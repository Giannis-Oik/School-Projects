# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HzNAL24E393zJnZdCqfZFGp44ZHH_9TP
"""

!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git
import sys; sys.path.insert(0, 'ColBERT/')

try: # When on google Colab, let's install all dependencies with pip.
    import google.colab
    !pip install -U pip
    !pip install -e ColBERT/['faiss-gpu','torch']
except Exception:
  import sys; sys.path.insert(0, 'ColBERT/')
  try:
    from colbert import Indexer, Searcher
  except Exception:
    print("If you're running outside Colab, please make sure you install ColBERT in conda following the instructions in our README. You can also install (as above) with pip but it may install slower or less stable faiss or torch dependencies. Conda is recommended.")
    assert False

import colbert
from colbert import Indexer, Searcher
from colbert.infra import Run, RunConfig, ColBERTConfig
from colbert.data import Queries, Collection

import pandas as pd
import os

text_list = [] #Listes gia th dhmiourgia twn katalhllwn dataframes
doc_list = []
query_id = []
query_text = []

for doc_id in range(1,1240): #Gia kathe doc_id se ayto to range anoixe ta arxeia me tetoio doc id efoson yparxoyn
  file_path = f'{doc_id:05}'+'.txt'

  exists = os.path.exists(file_path) #True an to arxeio yparxei
  if exists:
    f = open(file_path,'r')

    text = f.read() #Diavase to arxeio
    result = " ".join(line.strip() for line in text.splitlines()) #Enwse tis polles grammes toy arxeioy se mia
    doc_list.append(doc_id) #Prosthese to doc_id sth lista twn id
    text_list.append(result) #Prosthese to keimeno toy arxeioy sth lista arxeiwn
    f.close()

doc_data = { #Ftiaxe ena dict to opoio exei doc_id ta stoixeia ths listas id kai text ta stoixeia keimenoy gia kathe doc_id
  "doc_id": doc_list,
  "text": text_list
}

doc_df = pd.DataFrame(doc_data) #Dhmioyrghse to dataframe

collection = [doc_df.loc[x]['text'] for x in range(len(doc_df))] #Ftiaxe to collection gia to Colbert mesw toy dataframe

f = open('Queries_20.txt', 'r') #Idia diadikasia me th dhmiourgia toy collection gia ta queries

text = f.readlines()

count = 1
for line in text:
  query_id.append(count)
  count += 1
  query_text.append(line)

f.close()

q_data = {
  "query_id": query_id,
  "text": query_text
}

q_df = pd.DataFrame(q_data)

queries = [q_df.loc[x]['text'] for x in range(len(q_df))]

f'Loaded {len(queries)} queries and {len(collection):,} passages' #Emfanise posa queries kai keimena fortothikan

print(collection[2]) #Paradeigma enos arxeioy kai enos query
print()
print(queries[13])
print()

nbits = 2   #Dhmiourgia index name gia ton index ths sylloghs, arithmo bit gia thn kwdikopoihsh kai maximum arithmos tokens gia kathe arxeio
doc_maxlen = 300

index_name = f'IR-2024.{nbits}bits'

checkpoint = 'colbert-ir/colbertv2.0'

with Run().context(RunConfig(nranks=1, experiment='notebook')): #Vasei parametrwn kane dhmiourghse to configuration toy colbert
    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4)


    indexer = Indexer(checkpoint=checkpoint, config=config) #Vasei toy config dhmioyrghse ton indexer
    indexer.index(name=index_name, collection=collection[:], overwrite=True) #Kane index olh thn syllogh

with Run().context(RunConfig(experiment='notebook')): #Dhmioyrghse ton searcher vasei toy index kai ths sylloghs
    searcher = Searcher(index=index_name, collection=collection)


for query_id in range(19): #Gia ola ta queries entos toy range
  query = queries[query_id]
  print(f"#> {query}") #Emfanise to query

  results = searcher.search(query, k=5) #Vres ta k docs ws results gia to query mesw toy searcher sth syllogh

  for passage_id, passage_rank, passage_score in zip(*results): #Gia kathe doc sto result emfanise to
      id = doc_df.loc[passage_id]['doc_id']
      print(f"\t [{passage_rank}] \t\t {id} \t\t{passage_score:.1f} \t\t {searcher.collection[passage_id]}")

